{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf6739dc-7468-44b2-b00c-3ca089958005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import subprocess\n",
    "import yaml\n",
    "\n",
    "from linkml.validator.loaders import TsvLoader\n",
    "from linkml.utils.schema_builder import SchemaBuilder\n",
    "\n",
    "from linkml_runtime.linkml_model import SlotDefinition\n",
    "from linkml_runtime import SchemaView\n",
    "\n",
    "from linkml_map.session import Session\n",
    "from linkml_map.transformer.object_transformer import ObjectTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc21578-0c34-4e2a-b7ed-ec6df8b24359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix malformed yaml\n",
    "def quote_expr_values(yaml_text):\n",
    "    def replacer(match):\n",
    "        indent = match.group(1)\n",
    "        value = match.group(2).strip()\n",
    "\n",
    "        if value.startswith('\"') or value.startswith(\"'\"):\n",
    "            return match.group(0)\n",
    "        if re.match(r'^[\\w{}\\s\\*\\+\\-/().]+$', value):\n",
    "            return f'{indent}expr: \"{value}\"'\n",
    "        return match.group(0)\n",
    "\n",
    "    pattern = re.compile(r'^(\\s*)expr:\\s+(.*)', re.MULTILINE)\n",
    "    return pattern.sub(replacer, yaml_text)\n",
    "\n",
    "# raw = Path(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/bdy_hgt.yaml\").read_text()\n",
    "# quoted_fixed = quote_expr_values(raw)\n",
    "# split_blocks = re.split(r'(?<=\\n)(?=^\\s*class_derivations:\\s*)', quoted_fixed, flags=re.MULTILINE)\n",
    "# parsed_docs = [yaml.safe_load(doc) for doc in split_blocks]\n",
    "# print(yaml.dump(parsed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b44903a-f516-43d8-83a3-fb63cd6732e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_value_quantity(documents):\n",
    "    updated_docs = []\n",
    "\n",
    "    for doc_index, doc in enumerate(documents):\n",
    "        cd = doc.get(\"class_derivations\", {})\n",
    "        for cls_name, cls_info in cd.items():\n",
    "            slot_derivs = cls_info.get(\"slot_derivations\", {})\n",
    "            populated_from = cls_info.get(\"populated_from\")\n",
    "\n",
    "            # Extract and clean up the value_decimal slot\n",
    "            value_decimal_entry = slot_derivs.pop(\"value_decimal\", None)\n",
    "            cleaned_value_decimal = None\n",
    "            if value_decimal_entry:\n",
    "                if isinstance(value_decimal_entry, dict):\n",
    "                    pf = value_decimal_entry.get(\"populated_from\")\n",
    "                    expr = value_decimal_entry.get(\"expr\")\n",
    "\n",
    "                    if pf and isinstance(pf, dict) and \"expr\" in pf:\n",
    "                        cleaned_value_decimal = {\"expr\": pf[\"expr\"]}\n",
    "                    elif expr:\n",
    "                        cleaned_value_decimal = {\"expr\": expr}\n",
    "                    elif pf is None:\n",
    "                        raise ValueError(f\"[Doc {doc_index}] `value_decimal` has an empty `populated_from:` and no `expr:`\")\n",
    "                    elif isinstance(pf, str):\n",
    "                        cleaned_value_decimal = {\"populated_from\": pf}\n",
    "                    else:\n",
    "                        raise ValueError(f\"[Doc {doc_index}] Malformed `value_decimal`: {value_decimal_entry}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"[Doc {doc_index}] Unexpected `value_decimal` format: {value_decimal_entry}\")\n",
    "\n",
    "            # Extract and clean up the value_quantity.unit slot\n",
    "            value_unit_entry = slot_derivs.pop(\"value_quantity.unit\", None)\n",
    "            cleaned_unit = None\n",
    "            if value_unit_entry:\n",
    "                if isinstance(value_unit_entry, dict):\n",
    "                    pf = value_unit_entry.get(\"populated_from\")\n",
    "                    expr = value_unit_entry.get(\"expr\")\n",
    "\n",
    "                    if pf and isinstance(pf, dict) and \"expr\" in pf:\n",
    "                        cleaned_unit = {\"expr\": pf[\"expr\"]}\n",
    "                    elif expr:\n",
    "                        cleaned_unit = {\"expr\": expr}\n",
    "                    elif pf is None:\n",
    "                        raise ValueError(f\"[Doc {doc_index}] `value_quantity.unit` has an empty `populated_from:` and no `expr:`\")\n",
    "                    elif isinstance(pf, str):\n",
    "                        cleaned_unit = {\"populated_from\": pf}\n",
    "                    else:\n",
    "                        raise ValueError(f\"[Doc {doc_index}] Malformed `value_quantity.unit`: {value_unit_entry}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"[Doc {doc_index}] Unexpected `value_quantity.unit` format: {value_unit_entry}\")\n",
    "\n",
    "            # If either was valid, repackage into a nested Quantity\n",
    "            if cleaned_value_decimal or cleaned_unit:\n",
    "                quantity_deriv = {\n",
    "                    \"class_derivations\": {\n",
    "                        \"Quantity\": {\n",
    "                            \"populated_from\": populated_from,\n",
    "                            \"slot_derivations\": {}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                if cleaned_value_decimal:\n",
    "                    quantity_deriv[\"class_derivations\"][\"Quantity\"][\"slot_derivations\"][\"value_decimal\"] = cleaned_value_decimal\n",
    "                if cleaned_unit:\n",
    "                    quantity_deriv[\"class_derivations\"][\"Quantity\"][\"slot_derivations\"][\"unit\"] = cleaned_unit\n",
    "\n",
    "                slot_derivs[\"value_quantity\"] = {\n",
    "                    \"object_derivations\": [quantity_deriv]\n",
    "                }\n",
    "\n",
    "        updated_docs.append(doc)\n",
    "\n",
    "    return updated_docs\n",
    "\n",
    "\n",
    "\n",
    "# refactored_docs = refactor_value_quantity(parsed_docs)\n",
    "\n",
    "# print(yaml.dump(refactored_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2ff0ecd-e8d0-419c-a4d4-c3f3e6b653b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_value_quantity(documents):\n",
    "    updated_docs = []\n",
    "\n",
    "    for doc_index, doc in enumerate(documents):\n",
    "        cd = doc.get(\"class_derivations\", {})\n",
    "        for cls_name, cls_info in cd.items():\n",
    "            slot_derivs = cls_info.get(\"slot_derivations\", {})\n",
    "            populated_from = cls_info.get(\"populated_from\")\n",
    "\n",
    "            # Extract and clean slots to move into Quantity\n",
    "            quantity_subslots = {}\n",
    "            for slot in [\"value_decimal\", \"value_concept\", \"value_integer\", \"value_quantity.unit\"]:\n",
    "                # Support nested key for value_quantity.unit\n",
    "                key_in_slot_derivs = slot if slot in slot_derivs else slot.split(\".\")[-1]\n",
    "                entry = slot_derivs.pop(slot, None) or slot_derivs.pop(key_in_slot_derivs, None)\n",
    "                if entry:\n",
    "                    if isinstance(entry, dict):\n",
    "                        pf = entry.get(\"populated_from\")\n",
    "                        expr = entry.get(\"expr\")\n",
    "\n",
    "                        if pf and isinstance(pf, dict) and \"expr\" in pf:\n",
    "                            quantity_subslots[slot.split(\".\")[-1]] = {\"expr\": pf[\"expr\"]}\n",
    "                        elif expr:\n",
    "                            quantity_subslots[slot.split(\".\")[-1]] = {\"expr\": expr}\n",
    "                        elif pf is None:\n",
    "                            raise ValueError(f\"[Doc {doc_index}] `{slot}` has an empty `populated_from:` and no `expr:`\")\n",
    "                        elif isinstance(pf, str):\n",
    "                            quantity_subslots[slot.split(\".\")[-1]] = {\"populated_from\": pf}\n",
    "                        else:\n",
    "                            raise ValueError(f\"[Doc {doc_index}] Malformed `{slot}`: {entry}\")\n",
    "                    else:\n",
    "                        raise ValueError(f\"[Doc {doc_index}] Unexpected `{slot}` format: {entry}\")\n",
    "\n",
    "            # If any were found, create a nested Quantity class derivation\n",
    "            if quantity_subslots:\n",
    "                quantity_deriv = {\n",
    "                    \"class_derivations\": {\n",
    "                        \"Quantity\": {\n",
    "                            \"populated_from\": populated_from,\n",
    "                            \"slot_derivations\": quantity_subslots\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                slot_derivs[\"value_quantity\"] = {\n",
    "                    \"object_derivations\": [quantity_deriv]\n",
    "                }\n",
    "\n",
    "        updated_docs.append(doc)\n",
    "\n",
    "    return updated_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05cf4540-06ce-498f-a45b-3eb7e3e4ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_populated_from_with_pht(documents, phv_to_pht):\n",
    "    import re\n",
    "\n",
    "    def find_first_phv_in_slot(slot_derivations):\n",
    "        for slot_value in slot_derivations.values():\n",
    "            if isinstance(slot_value, dict):\n",
    "                pf = slot_value.get(\"populated_from\")\n",
    "                expr = slot_value.get(\"expr\")\n",
    "\n",
    "                if isinstance(pf, str) and pf.startswith(\"phv\"):\n",
    "                    return pf\n",
    "                if isinstance(expr, str):\n",
    "                    match = re.search(r\"(phv\\d{8})\", expr)\n",
    "                    if match:\n",
    "                        return match.group(1)\n",
    "        return None\n",
    "\n",
    "    def update_class_derivations(cls_derivations, doc_index, context=\"root\"):\n",
    "        for cls_name, cls_info in cls_derivations.items():\n",
    "            slot_derivs = cls_info.get(\"slot_derivations\", {})\n",
    "            pf = cls_info.get(\"populated_from\")\n",
    "\n",
    "            if pf == \"FHS\":\n",
    "                phv = find_first_phv_in_slot(slot_derivs)\n",
    "                if phv and phv in phv_to_pht:\n",
    "                    new_pf = phv_to_pht[phv]\n",
    "                    cls_info[\"populated_from\"] = new_pf\n",
    "                    # print(f\"✅ Updated {context}.{cls_name} populated_from: {phv} -> {new_pf}\")\n",
    "                else:\n",
    "                    print(f\"⚠️ Warning: No matching phv for {context}.{cls_name} in doc {doc_index}\")\n",
    "\n",
    "            # Recurse into nested object_derivations\n",
    "            for slot_name, slot_value in slot_derivs.items():\n",
    "                if isinstance(slot_value, dict) and \"object_derivations\" in slot_value:\n",
    "                    for obj in slot_value[\"object_derivations\"]:\n",
    "                        inner_cls_derivs = obj.get(\"class_derivations\")\n",
    "                        if inner_cls_derivs:\n",
    "                            update_class_derivations(inner_cls_derivs, doc_index, context=f\"{context}.{cls_name}.{slot_name}\")\n",
    "\n",
    "    for doc_index, doc in enumerate(documents):\n",
    "        top_cd = doc.get(\"class_derivations\", {})\n",
    "        update_class_derivations(top_cd, doc_index)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def load_phv_to_pht_map(file_path):\n",
    "    with open(file_path) as f:\n",
    "        return dict(line.strip().split(\": \") for line in f if line.strip())\n",
    "\n",
    "# for phv in $(grep -ho 'phv[0-9]\\{8\\}' bdy_hgt.yaml | sort -u); do grep -l \"$phv\" ../../../output/FHS_v31_c1/*.tsv \\\n",
    "# | sed -E \"s|.*/(pht[0-9]{6,}).tsv|$phv: \\1|\"; done > phv_to_pht.txt\n",
    "\n",
    "# phv_to_pht = load_phv_to_pht_map(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/phv_to_pht.txt\")\n",
    "\n",
    "# phv_to_pht = {\n",
    "#     \"phv00000680\": \"pht000009\",\n",
    "#     \"phv00001036\": \"pht000009\",\n",
    "#     \"phv00001367\": \"pht000012\",\n",
    "#     \"phv00001559\": \"pht000012\",\n",
    "#     \"phv00002207\": \"pht000016\",\n",
    "#     \"phv00002425\": \"pht000016\",\n",
    "# }\n",
    "\n",
    "# pht006027\n",
    "\n",
    "\n",
    "# pht_replace_docs = update_populated_from_with_pht(refactored_docs, phv_to_pht)\n",
    "\n",
    "# Dump to YAML\n",
    "# with open(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS-ingest/\" + \"bdy_hgt\" + \".yaml\", \"w\") as f:\n",
    "#     yaml.dump(pht_replace_docs, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "# print(yaml.dump(pht_replace_docs))\n",
    "# print(\"Success!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f126a7-96ea-4206-aabb-8bc1363e78ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = Path(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/bdy_hgt.yaml\").read_text()\n",
    "quoted_fixed = quote_expr_values(raw)\n",
    "split_blocks = re.split(r'(?<=\\n)(?=^\\s*class_derivations:\\s*)', quoted_fixed, flags=re.MULTILINE)\n",
    "parsed_docs = [yaml.safe_load(doc) for doc in split_blocks]\n",
    "\n",
    "refactored_docs = refactor_value_quantity(parsed_docs)\n",
    "\n",
    "phv_to_pht = load_phv_to_pht_map(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/phv_to_pht.txt\")\n",
    "\n",
    "pht_replace_docs = update_populated_from_with_pht(refactored_docs, phv_to_pht)\n",
    "\n",
    "# Dump to YAML\n",
    "with open(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS-ingest/\" + \"bdy_hgt\" + \".yaml\", \"w\") as f:\n",
    "    yaml.dump(pht_replace_docs, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "375c25d4-2729-4fba-a018-d933f47cbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base name\n",
    "spec_dir = \"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS\"\n",
    "base = \"afib\"\n",
    "data_dir = \"output/FHS_v31_c1\"\n",
    "\n",
    "# Run the shell command to regenerate phv_to_pht.txt\n",
    "subprocess.run(\n",
    "    f\"\"\"for phv in $(grep -ho 'phv[0-9]\\\\{{8\\\\}}' {spec_dir}/{base}.yaml | sort -u); do \\\n",
    "    grep -l \"$phv\" {data_dir}/*.tsv | \\\n",
    "    sed -E \"s|.*/(pht[0-9]{{6,}}).tsv|$phv: \\\\1|\"; done > phv_to_pht.txt\"\"\",\n",
    "    shell=True, check=True,\n",
    ")\n",
    "\n",
    "# Load and process the YAML\n",
    "raw = Path(f\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/{base}.yaml\").read_text()\n",
    "quoted_fixed = quote_expr_values(raw)\n",
    "split_blocks = re.split(r'(?<=\\n)(?=^\\s*class_derivations:\\s*)', quoted_fixed, flags=re.MULTILINE)\n",
    "parsed_docs = [yaml.safe_load(doc) for doc in split_blocks]\n",
    "\n",
    "refactored_docs = refactor_value_quantity(parsed_docs)\n",
    "\n",
    "phv_to_pht = load_phv_to_pht_map(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/phv_to_pht.txt\")\n",
    "\n",
    "pht_replace_docs = update_populated_from_with_pht(refactored_docs, phv_to_pht)\n",
    "\n",
    "# Dump to YAML\n",
    "with open(f\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS-ingest/{base}.yaml\", \"w\") as f:\n",
    "    yaml.dump(pht_replace_docs, f, sort_keys=False, allow_unicode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1995fd78-b9a7-4760-ab4a-fe08e3499b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icam\n",
      "insulin_in_blood\n"
     ]
    },
    {
     "ename": "ScannerError",
     "evalue": "while scanning an alias\n  in \"<unicode string>\", line 6, column 25:\n            populated_from: ***\n                            ^\nexpected alphabetic or numeric character, but found '*'\n  in \"<unicode string>\", line 6, column 26:\n            populated_from: ***\n                             ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mScannerError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m quoted_fixed = quote_expr_values(raw)\n\u001b[32m     28\u001b[39m split_blocks = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?<=\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn)(?=^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*class_derivations:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*)\u001b[39m\u001b[33m'\u001b[39m, quoted_fixed, flags=re.MULTILINE)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m parsed_docs = \u001b[43m[\u001b[49m\u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_blocks\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     31\u001b[39m refactored_docs = refactor_value_quantity(parsed_docs)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# phv_to_pht = load_phv_to_pht_map(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/phv_to_pht.txt\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     27\u001b[39m quoted_fixed = quote_expr_values(raw)\n\u001b[32m     28\u001b[39m split_blocks = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(?<=\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mn)(?=^\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*class_derivations:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*)\u001b[39m\u001b[33m'\u001b[39m, quoted_fixed, flags=re.MULTILINE)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m parsed_docs = [\u001b[43myaml\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m split_blocks]\n\u001b[32m     31\u001b[39m refactored_docs = refactor_value_quantity(parsed_docs)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# phv_to_pht = load_phv_to_pht_map(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/phv_to_pht.txt\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/__init__.py:125\u001b[39m, in \u001b[36msafe_load\u001b[39m\u001b[34m(stream)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_load\u001b[39m(stream):\n\u001b[32m    118\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[33;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m \u001b[33;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/__init__.py:81\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(stream, Loader)\u001b[39m\n\u001b[32m     79\u001b[39m loader = Loader(stream)\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     loader.dispose()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/constructor.py:49\u001b[39m, in \u001b[36mBaseConstructor.get_single_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_single_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# Ensure that the stream contains a single document and construct it.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_single_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.construct_document(node)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:36\u001b[39m, in \u001b[36mComposer.get_single_node\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     34\u001b[39m document = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(StreamEndEvent):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     document = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Ensure that the stream contains no more documents.\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(StreamEndEvent):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:55\u001b[39m, in \u001b[36mComposer.compose_document\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mself\u001b[39m.get_event()\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Compose the root node.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Drop the DOCUMENT-END event.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.get_event()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:84\u001b[39m, in \u001b[36mComposer.compose_node\u001b[39m\u001b[34m(self, parent, index)\u001b[39m\n\u001b[32m     82\u001b[39m     node = \u001b[38;5;28mself\u001b[39m.compose_sequence_node(anchor)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(MappingStartEvent):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.ascend_resolver()\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:133\u001b[39m, in \u001b[36mComposer.compose_mapping_node\u001b[39m\u001b[34m(self, anchor)\u001b[39m\n\u001b[32m    129\u001b[39m item_key = \u001b[38;5;28mself\u001b[39m.compose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m item_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[32m    135\u001b[39m node.value.append((item_key, item_value))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:84\u001b[39m, in \u001b[36mComposer.compose_node\u001b[39m\u001b[34m(self, parent, index)\u001b[39m\n\u001b[32m     82\u001b[39m     node = \u001b[38;5;28mself\u001b[39m.compose_sequence_node(anchor)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(MappingStartEvent):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.ascend_resolver()\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:133\u001b[39m, in \u001b[36mComposer.compose_mapping_node\u001b[39m\u001b[34m(self, anchor)\u001b[39m\n\u001b[32m    129\u001b[39m item_key = \u001b[38;5;28mself\u001b[39m.compose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m item_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[32m    135\u001b[39m node.value.append((item_key, item_value))\n",
      "    \u001b[31m[... skipping similar frames: Composer.compose_mapping_node at line 133 (2 times), Composer.compose_node at line 84 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:84\u001b[39m, in \u001b[36mComposer.compose_node\u001b[39m\u001b[34m(self, parent, index)\u001b[39m\n\u001b[32m     82\u001b[39m     node = \u001b[38;5;28mself\u001b[39m.compose_sequence_node(anchor)\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_event(MappingStartEvent):\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     node = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_mapping_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mself\u001b[39m.ascend_resolver()\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m node\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:133\u001b[39m, in \u001b[36mComposer.compose_mapping_node\u001b[39m\u001b[34m(self, anchor)\u001b[39m\n\u001b[32m    129\u001b[39m item_key = \u001b[38;5;28mself\u001b[39m.compose_node(node, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m#if item_key in node.value:\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m#    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m#            \"found duplicate key\", key_event.start_mark)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m item_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompose_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m#node.value[item_key] = item_value\u001b[39;00m\n\u001b[32m    135\u001b[39m node.value.append((item_key, item_value))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/composer.py:64\u001b[39m, in \u001b[36mComposer.compose_node\u001b[39m\u001b[34m(self, parent, index)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompose_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, parent, index):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAliasEvent\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     65\u001b[39m         event = \u001b[38;5;28mself\u001b[39m.get_event()\n\u001b[32m     66\u001b[39m         anchor = event.anchor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/parser.py:98\u001b[39m, in \u001b[36mParser.check_event\u001b[39m\u001b[34m(self, *choices)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         \u001b[38;5;28mself\u001b[39m.current_event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.current_event \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/parser.py:449\u001b[39m, in \u001b[36mParser.parse_block_mapping_value\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.check_token(ValueToken):\n\u001b[32m    448\u001b[39m     token = \u001b[38;5;28mself\u001b[39m.get_token()\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheck_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeyToken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mValueToken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBlockEndToken\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    450\u001b[39m         \u001b[38;5;28mself\u001b[39m.states.append(\u001b[38;5;28mself\u001b[39m.parse_block_mapping_key)\n\u001b[32m    451\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parse_block_node_or_indentless_sequence()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/scanner.py:116\u001b[39m, in \u001b[36mScanner.check_token\u001b[39m\u001b[34m(self, *choices)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_token\u001b[39m(\u001b[38;5;28mself\u001b[39m, *choices):\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# Check if the next token is one of the given types.\u001b[39;00m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.need_more_tokens():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_more_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokens:\n\u001b[32m    118\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m choices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/scanner.py:227\u001b[39m, in \u001b[36mScanner.fetch_more_tokens\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Is it an alias?\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ch == \u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfetch_alias\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Is it an anchor?\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ch == \u001b[33m'\u001b[39m\u001b[33m&\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/scanner.py:610\u001b[39m, in \u001b[36mScanner.fetch_alias\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    607\u001b[39m \u001b[38;5;28mself\u001b[39m.allow_simple_key = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    609\u001b[39m \u001b[38;5;66;03m# Scan and add ALIAS.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m \u001b[38;5;28mself\u001b[39m.tokens.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscan_anchor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAliasToken\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/sbgenomics/workspace/dm-bip/.venv/lib/python3.11/site-packages/yaml/scanner.py:922\u001b[39m, in \u001b[36mScanner.scan_anchor\u001b[39m\u001b[34m(self, TokenClass)\u001b[39m\n\u001b[32m    920\u001b[39m     ch = \u001b[38;5;28mself\u001b[39m.peek(length)\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m length:\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ScannerError(\u001b[33m\"\u001b[39m\u001b[33mwhile scanning an \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % name, start_mark,\n\u001b[32m    923\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexpected alphabetic or numeric character, but found \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    924\u001b[39m             % ch, \u001b[38;5;28mself\u001b[39m.get_mark())\n\u001b[32m    925\u001b[39m value = \u001b[38;5;28mself\u001b[39m.prefix(length)\n\u001b[32m    926\u001b[39m \u001b[38;5;28mself\u001b[39m.forward(length)\n",
      "\u001b[31mScannerError\u001b[39m: while scanning an alias\n  in \"<unicode string>\", line 6, column 25:\n            populated_from: ***\n                            ^\nexpected alphabetic or numeric character, but found '*'\n  in \"<unicode string>\", line 6, column 26:\n            populated_from: ***\n                             ^"
     ]
    }
   ],
   "source": [
    "spec_dir = Path(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS\")\n",
    "data_dir = \"output/FHS_v31_c1\"\n",
    "output_dir = Path(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS-ingest\")\n",
    "\n",
    "for yaml_file in spec_dir.glob(\"*.yaml\"):\n",
    "    base = yaml_file.stem  # Strip .yaml\n",
    "\n",
    "    # Not completed: cig_smoke, creat_bld, edu_lvl, fam_income, fast_gluc_bld, glucose_bld, hdl, hip_circ\n",
    "    #                hist_cor_angio, hist_cor_bypg, hist_my_inf, hrtrt, hypertension, insulin_in_blood\n",
    "    start_at = \"icam\"\n",
    "    if base < start_at:\n",
    "        continue\n",
    "    \n",
    "    print(base)\n",
    "\n",
    "    # Run the shell command to regenerate phv_to_pht.txt\n",
    "    result = subprocess.run(\n",
    "        f\"\"\"for phv in $(grep -ho 'phv[0-9]\\\\{{8\\\\}}' {spec_dir}/{base}.yaml | sort -u); do \\\n",
    "        grep -l \"$phv\" {data_dir}/*.tsv | \\\n",
    "        sed -E \"s|.*/(pht[0-9]{{6,}}).tsv|$phv: \\\\1|\"; done\"\"\",\n",
    "        shell=True, check=True, capture_output=True, text=True,\n",
    "    )\n",
    "    phv_to_pht = dict(line.split(\": \") for line in result.stdout.strip().splitlines())\n",
    "\n",
    "    # Load and process the YAML\n",
    "    raw = yaml_file.read_text()\n",
    "    quoted_fixed = quote_expr_values(raw)\n",
    "    split_blocks = re.split(r'(?<=\\n)(?=^\\s*class_derivations:\\s*)', quoted_fixed, flags=re.MULTILINE)\n",
    "    parsed_docs = [yaml.safe_load(doc) for doc in split_blocks]\n",
    "\n",
    "    refactored_docs = refactor_value_quantity(parsed_docs)\n",
    "\n",
    "    # phv_to_pht = load_phv_to_pht_map(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/phv_to_pht.txt\")\n",
    "\n",
    "    pht_replace_docs = update_populated_from_with_pht(refactored_docs, phv_to_pht)\n",
    "\n",
    "    # Dump to YAML\n",
    "    output_file = f\"{output_dir}/{base}.yaml\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        yaml.dump(pht_replace_docs, f, sort_keys=False, allow_unicode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f59373-e14f-48ab-9472-5e560e202d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person - top level class\n",
    "person_yaml = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Person:\n",
    "    populated_from: pht000009\n",
    "    slot_derivations:\n",
    "      species:\n",
    "        expr: \"'Homo Sapiens'\"\n",
    "      identity:\n",
    "        populated_from: dbGaP_Subject_ID\n",
    "\"\"\")\n",
    "\n",
    "# # Dump to YAML\n",
    "# with open(var_dir + \"person\" + \".yaml\", \"w\") as f:\n",
    "#     yaml.dump(person_yaml, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6956b1d2-bcc6-47f8-8b35-763c7074f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sv = SchemaView(\"/sbgenomics/workspace/output/Schema_FHS_v31_c1/schema-automator-data/Schema_FHS_v31_c1.yaml\")\n",
    "source_schema = source_sv.schema\n",
    "\n",
    "target_sv = SchemaView(\"NHLBI-BDC-DMC-HM/src/bdchm/schema/bdchm.yaml\")\n",
    "target_schema = target_sv.schema\n",
    "\n",
    "var_dir = \"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876642b2-8b28-4e39-8b12-4ca7822cc0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'species': 'Homo Sapiens', 'identity': [16956]}\n",
      "Transformation Successful!\n"
     ]
    }
   ],
   "source": [
    "transform_spec = person_yaml\n",
    "\n",
    "data_loader = TsvLoader(\"/sbgenomics/workspace/output/FHS_v31_c1/pht000009.tsv\")\n",
    "data_rows = data_loader.iter_instances()\n",
    "\n",
    "first_row = next(data_rows)\n",
    "cur_row = first_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(cur_row, source_type=\"pht000009\")\n",
    "\n",
    "print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce73026a-6662-4abe-bfbc-5c737024a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant - top level class for study data\n",
    "participant_yaml = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Participant:\n",
    "    populated_from: pht000009\n",
    "    slot_derivations:\n",
    "      # associated_participant: \n",
    "      #   populated_from: phv00007675\n",
    "      identity:\n",
    "        populated_from: dbGaP_Subject_ID\n",
    "      member_of_research_study:\n",
    "        expr: \"'FHS'\"\n",
    "\"\"\")\n",
    "\n",
    "# # Dump to YAML\n",
    "# with open(var_dir + \"participant\" + \".yaml\", \"w\") as f:\n",
    "#     yaml.dump(participant_yaml, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c43d7ea-4564-48c3-a2a6-3c837e0b14dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_derivations': {'MeasurementObservation': {'populated_from': 'pht000009',\n",
       "    'slot_derivations': {'associated_participant': {'populated_from': 'phv00001036'},\n",
       "     'associated_visit': {'expr': \"'FHS ORIGINAL EXAM 4'\"},\n",
       "     'observation_type': {'expr': \"'OBA:VT0001253'\"},\n",
       "     'value_quantity': {'object_derivations': [{'class_derivations': {'Quantity': {'populated_from': 'pht000009',\n",
       "          'slot_derivations': {'value_decimal': {'expr': '{phv00000680} * 2.54'},\n",
       "           'unit': {'expr': \"'cm'\"}}}}}]}}}}},\n",
       " {'class_derivations': {'MeasurementObservation': {'populated_from': 'pht000009',\n",
       "    'slot_derivations': {'associated_participant': {'populated_from': 'phv00001036'},\n",
       "     'associated_visit': {'expr': \"'FHS ORIGINAL EXAM 1'\"},\n",
       "     'observation_type': {'expr': \"'OBA:VT0001253'\"},\n",
       "     'value_quantity': {'object_derivations': [{'class_derivations': {'Quantity': {'populated_from': 'pht000009',\n",
       "          'slot_derivations': {'value_decimal': {'expr': '{phv00000539} * 2.54'},\n",
       "           'unit': {'expr': \"'cm'\"}}}}}]}}}}},\n",
       " {'class_derivations': {'MeasurementObservation': {'populated_from': 'pht000009',\n",
       "    'slot_derivations': {'associated_participant': {'populated_from': 'phv00001036'},\n",
       "     'associated_visit': {'expr': \"'FHS ORIGINAL EXAM 5'\"},\n",
       "     'observation_type': {'expr': \"'OBA:VT0001253'\"},\n",
       "     'value_quantity': {'object_derivations': [{'class_derivations': {'Quantity': {'populated_from': 'pht000009',\n",
       "          'slot_derivations': {'value_decimal': {'expr': '{phv00000744} * 2.54'},\n",
       "           'unit': {'expr': \"'cm'\"}}}}}]}}}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MeasurementObservation class derivations\n",
    "bdy_hgt = refactored_docs\n",
    "# bdy_hgt = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bdy_hgt\" + \".yaml\")))\n",
    "# bdy_wgt = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bdy_wgt\" + \".yaml\")))\n",
    "# bmi = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bmi\" + \".yaml\")))\n",
    "# bp_diastolic = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bp_diastolic\" + \".yaml\")))\n",
    "# bp_systolic = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bp_systolic\" + \".yaml\")))\n",
    "# fev1 = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"fev1\" + \".yaml\")))\n",
    "# fev1_fvc = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"fev1_fvc\" + \".yaml\")))\n",
    "# fvc = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"fvc\" + \".yaml\")))\n",
    "# hrt_rt = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"hrt_rt\" + \".yaml\")))\n",
    "# spo2 = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"spo2\" + \".yaml\")))\n",
    "\n",
    "# Get the demography slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_exposures_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"exposures\", {})\n",
    "\n",
    "# Add the Demography object_derivation to the demography slot\n",
    "participant_exposures_slot.setdefault(\"object_derivations\", bdy_hgt)\n",
    "# participant_exposures_slot.setdefault(\"object_derivations\", [\n",
    "#     bdy_hgt,\n",
    "#     tak_betablk,\n",
    "#     tak_adrenergics,\n",
    "#     tak_cort_steroid_resp,\n",
    "#     tak_cort_steroid_oral,\n",
    "#     tak_anabolic_steroid,\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d38cb8-ced7-46d3-8685-84d100e98a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identity': [16956], 'member_of_research_study': 'FHS', 'exposures': [{'associated_participant': 1, 'associated_visit': 'FHS ORIGINAL EXAM 4', 'observation_type': 'OBA:VT0001253', 'value_quantity': {'value_decimal': 162.56, 'unit': 'cm'}}, {'associated_participant': 1, 'associated_visit': 'FHS ORIGINAL EXAM 1', 'observation_type': 'OBA:VT0001253', 'value_quantity': {'value_decimal': 162.56, 'unit': 'cm'}}, {'associated_participant': 1, 'associated_visit': 'FHS ORIGINAL EXAM 5', 'observation_type': 'OBA:VT0001253', 'value_quantity': {'value_decimal': 162.56, 'unit': 'cm'}}]}\n",
      "Transformation Successful!\n"
     ]
    }
   ],
   "source": [
    "transform_spec = participant_yaml\n",
    "\n",
    "input_data = cur_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(input_data, source_type=\"pht000009\")\n",
    "\n",
    "# print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd2ea45-fd3a-41cf-b3c7-b35599939f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_derivations': {'Condition': {'populated_from': 'pht000030',\n",
       "    'slot_derivations': {'associated_participant': {'populated_from': 'phv00056635'},\n",
       "     'associated_visit': {'expr': \"'FHS OFFSPRING BASELINE'\"},\n",
       "     'condition_concept': {'expr': \"'HP:0001681'\"},\n",
       "     'condition_status': {'populated_from': 'phv00055298',\n",
       "      'value_mappings': {'0': 'ABSENT', '1': 'PRESENT', '8': 'UNKNOWN'}},\n",
       "     'condition_provenance': {'expr': \"'CLINICAL_DIAGNOSIS'\"},\n",
       "     'relationship_to_participant': {'expr': \"'ONESELF'\"}}}}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's start with one simple condition\n",
    "angina = yaml.safe_load(open(str(var_dir + \"angina\" + \".yaml\")))\n",
    "# print(yaml.dump(angina))\n",
    "\n",
    "# Get the conditions slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_exposures_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"conditions\", {})\n",
    "\n",
    "# Add the conditions object_derivation to the demography slot\n",
    "participant_exposures_slot.setdefault(\"object_derivations\", [\n",
    "    angina,\n",
    "    # asthma,\n",
    "    # copd,\n",
    "    # diabetes,\n",
    "    # hist_hrt_failure,\n",
    "    # hist_my_inf,\n",
    "    # hyperten,\n",
    "    # pad,\n",
    "    # slp_ap,\n",
    "    # stroke,\n",
    "    # stroke_isch_atk,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03bf4dbd-14f8-466e-828b-2d9ff79f24bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'identity': [16957], 'member_of_research_study': 'FHS', 'conditions': [{'associated_participant': None, 'associated_visit': 'FHS OFFSPRING BASELINE', 'condition_concept': 'HP:0001681', 'condition_status': None, 'condition_provenance': 'CLINICAL_DIAGNOSIS', 'relationship_to_participant': 'ONESELF'}]}\n",
      "Transformation Successful!\n"
     ]
    }
   ],
   "source": [
    "transform_spec = participant_yaml\n",
    "\n",
    "input_data = cur_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(input_data, source_type=\"pht000030\")\n",
    "\n",
    "print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de820310-8052-4865-bee4-4c48b647f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_spec = refactored_docs\n",
    "\n",
    "\n",
    "data_loader = TsvLoader(\"/sbgenomics/workspace/output/FHS_v31_c1/pht000009.tsv\")\n",
    "data_rows = data_loader.iter_instances()\n",
    "\n",
    "first_row = next(data_rows)\n",
    "cur_row = first_row\n",
    "\n",
    "input_data = cur_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(input_data, source_type=\"pht000009\")\n",
    "\n",
    "print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c7e83-ac94-43fb-872d-9fa7fa3b366c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c44f9e6-2a70-495f-bc23-92fcccc10ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sequential class_derivations (malformed yaml) into list of class derivations.\n",
    "\n",
    "# Function to fix unquoted expr: text (malformed yaml)\n",
    "def quote_expr_values(yaml_text):\n",
    "    def replacer(match):\n",
    "        indent = match.group(1)\n",
    "        value = match.group(2).strip()\n",
    "\n",
    "        # Don't quote if already quoted OR looks like a quoted literal\n",
    "        if value.startswith('\"') or value.startswith(\"'\"):\n",
    "            return match.group(0)\n",
    "\n",
    "        # Don't quote if it's a simple scalar (e.g., a single variable)\n",
    "        if re.match(r'^[\\w{}\\s\\*\\+\\-/().]+$', value):\n",
    "            return f'{indent}expr: \"{value}\"'\n",
    "\n",
    "        # Otherwise, leave it as-is\n",
    "        return match.group(0)\n",
    "\n",
    "    pattern = re.compile(r'^(\\s*)expr:\\s+(.*)', re.MULTILINE)\n",
    "    return pattern.sub(replacer, yaml_text)\n",
    "\n",
    "# Read the raw YAML as text\n",
    "raw = Path(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/bdy_hgt_test.yaml\").read_text()\n",
    "# print(raw)\n",
    "\n",
    "quoted_fixed = quote_expr_values(raw)\n",
    "# Split while *keeping* 'class_derivations:' in each result, skip first line\n",
    "split_blocks = re.split(r'(?<=\\n)(?=^\\s*class_derivations:\\s*)', quoted_fixed, flags=re.MULTILINE)\n",
    "# print(split_blocks)\n",
    "\n",
    "# print(split_blocks)\n",
    "parsed_docs = [yaml.safe_load(doc) for doc in split_blocks]\n",
    "print(yaml.dump(parsed_docs))\n",
    "\n",
    "\n",
    "raw2 = Path(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/bdy_hgt_test2.yaml\").read_text()\n",
    "# print(raw2)\n",
    "\n",
    "# Split while *keeping* 'class_derivations:' in each result, skip first line\n",
    "split_blocks2 = re.split(r'(?<=\\n)(?=^\\s*class_derivations:\\s*)', raw2, flags=re.MULTILINE)\n",
    "# print(split_blocks2)\n",
    "print(split_blocks == split_blocks2)\n",
    "# quoted_fixed2 = quote_expr_values(raw2)\n",
    "# print(quoted_fixed2)\n",
    "parsed_docs2 = [yaml.safe_load(doc) for doc in split_blocks2]\n",
    "# print(yaml.dump(parsed_docs2))\n",
    "\n",
    "# parsed_docs = [yaml.safe_load(doc) for doc in quoted_fixed]\n",
    "\n",
    "# Use these if we can't guarantee the first class_derivation is first line of file.\n",
    "# # Split while *keeping* 'class_derivations:' in each result\n",
    "# split_blocks = re.split(r'(?=^\\s*class_derivations:\\s*)', raw, flags=re.MULTILINE)\n",
    "# # First block is likely empty or comments/header — skip it\n",
    "# blocks = [b for b in split_blocks[1:]]\n",
    "# parsed_docs = [yaml.safe_load(doc) for doc in blocks]\n",
    "\n",
    "# print(yaml.dump(quoted_fixed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb0f83-db6c-42ca-a11f-66a01b5203f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn sequential class_derivations (malformed yaml) into list of class derivations.\n",
    "\n",
    "# Read the raw YAML as text\n",
    "raw = Path(\"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/apnea_hypop_index.yaml\").read_text()\n",
    "print(raw)\n",
    "\n",
    "# Split while *keeping* 'class_derivations:' in each result, skip first line\n",
    "split_blocks = re.split(r'(?<=\\n)(?=^\\s*class_derivations:\\s*)', raw, flags=re.MULTILINE)\n",
    "parsed_docs = [yaml.safe_load(doc) for doc in split_blocks]\n",
    "\n",
    "# Use these if we can't guarantee the first class_derivation is first line of file.\n",
    "# # Split while *keeping* 'class_derivations:' in each result\n",
    "# split_blocks = re.split(r'(?=^\\s*class_derivations:\\s*)', raw, flags=re.MULTILINE)\n",
    "# # First block is likely empty or comments/header — skip it\n",
    "# blocks = [b for b in split_blocks[1:]]\n",
    "# parsed_docs = [yaml.safe_load(doc) for doc in blocks]\n",
    "\n",
    "print(yaml.dump(parsed_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50907a8-faba-41aa-866d-4d793ad177b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "class LazySubjectDict(dict):\n",
    "    \"\"\"\n",
    "    Lazily loads per-pht data for a single subject on demand.\n",
    "    \"\"\"\n",
    "    def __init__(self, subject_id, data_dir):\n",
    "        super().__init__()\n",
    "        self.subject_id = subject_id\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self._cache = {}\n",
    "\n",
    "    def __getitem__(self, pht_id):\n",
    "        if pht_id in self._cache:\n",
    "            return self._cache[pht_id]\n",
    "\n",
    "        file_path = self.data_dir / f\"{pht_id}.tsv\"\n",
    "        if not file_path.exists():\n",
    "            raise KeyError(f\"No such file: {file_path}\")\n",
    "\n",
    "        with open(file_path, newline='') as f:\n",
    "            reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "            for row in reader:\n",
    "                if row.get(\"dbGaP_Subject_ID\") == self.subject_id:\n",
    "                    self._cache[pht_id] = row\n",
    "                    return row\n",
    "\n",
    "        raise KeyError(f\"Subject {self.subject_id} not found in {pht_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef9c58-9bce-420c-9248-7c0f9a64ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sv = SchemaView(\"/sbgenomics/workspace/output/Schema_FHS_v31_c1/schema-automator-data/Schema_FHS_v31_c1.yaml\")\n",
    "source_schema = source_sv.schema\n",
    "\n",
    "target_sv = SchemaView(\"NHLBI-BDC-DMC-HM/src/bdchm/schema/bdchm.yaml\")\n",
    "target_schema = target_sv.schema\n",
    "\n",
    "data_loader = TsvLoader(\"/sbgenomics/workspace/output/FHS_v31_c1/pht000030.tsv\")\n",
    "data_rows = data_loader.iter_instances()\n",
    "\n",
    "first_row = next(data_rows)\n",
    "cur_row = first_row\n",
    "\n",
    "var_dir = \"NHLBI-BDC-DMC-HV/priority_variables_transform/FHS/\"\n",
    "print(cur_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd0c51-db7e-455b-8a76-0e650078f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person - top level class\n",
    "person_yaml = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Person:\n",
    "    populated_from: pht000030\n",
    "    slot_derivations:\n",
    "      species:\n",
    "        expr: \"'Homo Sapiens'\"\n",
    "      identity:\n",
    "        populated_from: dbGaP_Subject_ID\n",
    "\"\"\")\n",
    "\n",
    "# # Dump to YAML\n",
    "# with open(var_dir + \"person\" + \".yaml\", \"w\") as f:\n",
    "#     yaml.dump(person_yaml, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2f07c-b3ff-4c79-afc4-cb4c14d566c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participant - top level class for study data\n",
    "participant_yaml = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Participant:\n",
    "    populated_from: pht000030\n",
    "    slot_derivations:\n",
    "      # associated_participant: \n",
    "      #   populated_from: phv00007675\n",
    "      identity:\n",
    "        populated_from: dbGaP_Subject_ID\n",
    "      member_of_research_study:\n",
    "        expr: \"'FHS'\"\n",
    "\"\"\")\n",
    "\n",
    "# # Dump to YAML\n",
    "# with open(var_dir + \"participant\" + \".yaml\", \"w\") as f:\n",
    "#     yaml.dump(participant_yaml, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930fcd2-e946-456b-b50f-4daa8af5ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_spec = participant_yaml\n",
    "\n",
    "input_data = cur_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(input_data, source_type=\"pht000030\")\n",
    "\n",
    "print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a70eb-fe22-4b65-b53e-f250ff391f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the participants slot\n",
    "person_class = person_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Person\", {})\n",
    "person_participants_slot = person_class.setdefault(\"slot_derivations\", {}).setdefault(\"participants\", {})\n",
    "\n",
    "# Add the Participant object_derivation to the participants slot\n",
    "person_participants_slot.setdefault(\"object_derivations\", [ participant_yaml ])\n",
    "\n",
    "# print(yaml.dump(person_yaml, sort_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209e7ddc-617e-4eee-b678-61ec490c1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_spec = person_yaml\n",
    "\n",
    "input_data = cur_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(input_data, source_type=\"pht000030\")\n",
    "\n",
    "print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5cb63d-1d40-450b-b4b8-d9454a45a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with one simple condition\n",
    "angina = yaml.safe_load(open(str(var_dir + \"angina\" + \".yaml\")))\n",
    "# print(yaml.dump(angina))\n",
    "\n",
    "# Get the conditions slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_exposures_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"conditions\", {})\n",
    "\n",
    "# Add the conditions object_derivation to the demography slot\n",
    "participant_exposures_slot.setdefault(\"object_derivations\", [\n",
    "    angina,\n",
    "    # asthma,\n",
    "    # copd,\n",
    "    # diabetes,\n",
    "    # hist_hrt_failure,\n",
    "    # hist_my_inf,\n",
    "    # hyperten,\n",
    "    # pad,\n",
    "    # slp_ap,\n",
    "    # stroke,\n",
    "    # stroke_isch_atk,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a51fe-3ba9-474b-aae5-6d903e1b18d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(yaml.dump(person_yaml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a1c2fc-e2a1-47cd-83a3-e6d257aff0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_spec = person_yaml\n",
    "\n",
    "data_loader = TsvLoader(\"/sbgenomics/workspace/output/FHS_v31_c1/pht000030.tsv\")\n",
    "data_rows = data_loader.iter_instances()\n",
    "\n",
    "first_row = next(data_rows)\n",
    "cur_row = first_row\n",
    "\n",
    "other_data_loader = TsvLoader(\"/sbgenomics/workspace/output/FHS_v31_c1/pht000395.tsv\")\n",
    "other_data_rows = other_data_loader.iter_instances()\n",
    "\n",
    "other_first_row = next(other_data_rows)\n",
    "other_cur_row = other_first_row\n",
    "\n",
    "input_data = {\n",
    "    \"pht000030\": cur_row,\n",
    "    \"pht000395\": other_cur_row\n",
    "}\n",
    "\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(input_data, source_type=\"FHS\")\n",
    "\n",
    "print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01721d-7268-4c24-9231-2707f85ba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Condition class derivations\n",
    "angina = yaml.safe_load(open(str(var_dir + \"condition/\" + \"angina\" + \".yaml\")))\n",
    "asthma = yaml.safe_load(open(str(var_dir + \"condition/\" + \"asthma\" + \".yaml\")))\n",
    "copd = yaml.safe_load(open(str(var_dir + \"condition/\" + \"copd\" + \".yaml\")))\n",
    "diabetes = yaml.safe_load(open(str(var_dir + \"condition/\" + \"diabetes\" + \".yaml\")))\n",
    "hist_hrt_failure = yaml.safe_load(open(str(var_dir + \"condition/\" + \"hist_hrt_failure\" + \".yaml\")))\n",
    "hist_my_inf = yaml.safe_load(open(str(var_dir + \"condition/\" + \"hist_my_inf\" + \".yaml\")))\n",
    "hyperten = yaml.safe_load(open(str(var_dir + \"condition/\" + \"hyperten\" + \".yaml\")))\n",
    "pad = yaml.safe_load(open(str(var_dir + \"condition/\" + \"pad\" + \".yaml\")))\n",
    "slp_ap = yaml.safe_load(open(str(var_dir + \"condition/\" + \"slp_ap\" + \".yaml\")))\n",
    "stroke = yaml.safe_load(open(str(var_dir + \"condition/\" + \"stroke\" + \".yaml\")))\n",
    "stroke_isch_atk = yaml.safe_load(open(str(var_dir + \"condition/\" + \"stroke_isch_atk\" + \".yaml\")))\n",
    "\n",
    "# Get the conditions slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_exposures_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"conditions\", {})\n",
    "\n",
    "# Add the conditions object_derivation to the demography slot\n",
    "participant_exposures_slot.setdefault(\"object_derivations\", [\n",
    "    angina,\n",
    "    asthma,\n",
    "    copd,\n",
    "    diabetes,\n",
    "    hist_hrt_failure,\n",
    "    hist_my_inf,\n",
    "    hyperten,\n",
    "    pad,\n",
    "    slp_ap,\n",
    "    stroke,\n",
    "    stroke_isch_atk,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7015ab-d611-4c13-8cbf-dc2822def350",
   "metadata": {},
   "outputs": [],
   "source": [
    "demography_yaml = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Demography:\n",
    "    populated_from: COPDGene\n",
    "    slot_derivations:\n",
    "      associated_participant:\n",
    "        populated_from: phv00159568\n",
    "      sex:\n",
    "        populated_from: phv00159571\n",
    "        value_mappings:\n",
    "          '1': OMOP:8507  # MALE\n",
    "          '2': OMOP:8532  # FEMALE\n",
    "      ethnicity:\n",
    "        populated_from: phv00159573\n",
    "        value_mappings:\n",
    "          '1': HISPANIC_OR_LATINO\n",
    "          '2': NOT_HISPANIC_OR_LATINO\n",
    "      race:\n",
    "        populated_from: phv00159572\n",
    "        value_mappings:\n",
    "          '1': OMOP:8527\n",
    "          '2': OMOP:8516\n",
    "          '3': OMOP:8515\n",
    "          '4': OMOP:8557\n",
    "          '5': OMOP:8657\n",
    "          '6': OMOP:45880900\n",
    "          '7': OMOP:8552\n",
    "\"\"\")\n",
    "\n",
    "# Dump to YAML\n",
    "with open(var_dir + \"demography\" + \".yaml\", \"w\") as f:\n",
    "    yaml.dump(demography_yaml, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d3105-77d7-433b-a16a-5ddb36660e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the demography slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_demography_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"demography\", {})\n",
    "\n",
    "# Add the Demography object_derivation to the demography slot\n",
    "participant_demography_slot.setdefault(\"object_derivations\", [ demography_yaml ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5ad67-f675-4164-ae89-ec6841e2d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DrugExposure class derivations\n",
    "tak_betablk_resp = yaml.safe_load(open(str(var_dir + \"exposure/\" + \"tak_betablk_resp\" + \".yaml\")))\n",
    "tak_betablk = yaml.safe_load(open(str(var_dir + \"exposure/\" + \"tak_betablk\" + \".yaml\")))\n",
    "tak_adrenergics = yaml.safe_load(open(str(var_dir + \"exposure/\" + \"tak_adrenergics\" + \".yaml\")))\n",
    "tak_cort_steroid_resp = yaml.safe_load(open(str(var_dir + \"exposure/\" + \"tak_cort_steroid_resp\" + \".yaml\")))\n",
    "tak_cort_steroid_oral = yaml.safe_load(open(str(var_dir + \"exposure/\" + \"tak_cort_steroid_oral\" + \".yaml\")))\n",
    "tak_anabolic_steroid = yaml.safe_load(open(str(var_dir + \"exposure/\" + \"tak_anabolic_steroid\" + \".yaml\")))\n",
    "\n",
    "# Get the demography slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_exposures_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"exposures\", {})\n",
    "\n",
    "# Add the Demography object_derivation to the demography slot\n",
    "participant_exposures_slot.setdefault(\"object_derivations\", [\n",
    "    tak_betablk_resp,\n",
    "    tak_betablk,\n",
    "    tak_adrenergics,\n",
    "    tak_cort_steroid_resp,\n",
    "    tak_cort_steroid_oral,\n",
    "    tak_anabolic_steroid,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7c770-d698-4a11-aaa4-0b371e8ea8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MeasurementObservation class derivations\n",
    "bdy_hgt = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bdy_hgt\" + \".yaml\")))\n",
    "bdy_wgt = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bdy_wgt\" + \".yaml\")))\n",
    "bmi = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bmi\" + \".yaml\")))\n",
    "bp_diastolic = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bp_diastolic\" + \".yaml\")))\n",
    "bp_systolic = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"bp_systolic\" + \".yaml\")))\n",
    "fev1 = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"fev1\" + \".yaml\")))\n",
    "fev1_fvc = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"fev1_fvc\" + \".yaml\")))\n",
    "fvc = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"fvc\" + \".yaml\")))\n",
    "hrt_rt = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"hrt_rt\" + \".yaml\")))\n",
    "spo2 = yaml.safe_load(open(str(var_dir + \"measurement_observation/\" + \"spo2\" + \".yaml\")))\n",
    "\n",
    "# Get the demography slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_exposures_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"exposures\", {})\n",
    "\n",
    "# Add the Demography object_derivation to the demography slot\n",
    "participant_exposures_slot.setdefault(\"object_derivations\", [\n",
    "    bdy_hgt,\n",
    "    tak_betablk,\n",
    "    tak_adrenergics,\n",
    "    tak_cort_steroid_resp,\n",
    "    tak_cort_steroid_oral,\n",
    "    tak_anabolic_steroid,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd1d49-f23d-4992-a0bd-f93241a510b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_yaml = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Observation:\n",
    "    populated_from: COPDGene\n",
    "    slot_derivations:\n",
    "      associated_participant:\n",
    "        populated_from: phv00159568\n",
    "      observation_type:\n",
    "        expr: \"'OMOP:4282779'\"  # Cigarette smoking status\n",
    "      value_enum:\n",
    "        expr: \"'OMOP:40766945' if {phv00159749} == 1 else 'OMOP:45883458' if {phv00159747} == 1 else 'OMOP:45883537'\"\n",
    "\"\"\")\n",
    "\n",
    "# Dump to YAML\n",
    "with open(var_dir + \"observation\" + \".yaml\", \"w\") as f:\n",
    "    yaml.dump(observation_yaml, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ae05d-a027-447a-9e3c-216ed11e308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the observations slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_observations_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"observations\", {})\n",
    "\n",
    "# Add the Demography object_derivation to the demography slot\n",
    "participant_observations_slot.setdefault(\"object_derivations\", [ observation_yaml ])\n",
    "\n",
    "# print(yaml.dump(person_yaml))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3705b06-a868-411d-91fe-273c2590a378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_cor_angio\n",
    "hist_cor_angio = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Procedure:\n",
    "    populated_from: COPDGene\n",
    "    slot_derivations:\n",
    "      associated_participant:\n",
    "        populated_from: phv00159568\n",
    "      procedure_concept:\n",
    "        expr: \"'OMOP:4184832'\"  # Coronary angioplasty\n",
    "      procedure_status:\n",
    "        populated_from: phv00159632\n",
    "        value_mappings:\n",
    "          '0': ABSENT\n",
    "          '1': PRESENT\n",
    "\"\"\")\n",
    "\n",
    "# Dump to YAML\n",
    "with open(var_dir + \"procedure/\" + \"hist_cor_angio\" + \".yaml\", \"w\") as f:\n",
    "    yaml.dump(hist_cor_angio, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881446b-f8a6-4948-a740-8f4c32f6052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_spec = hist_cor_angio\n",
    "\n",
    "input_data = cur_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "result = transformer.map_object(input_data, source_type=\"COPDGene\")\n",
    "\n",
    "print(result)\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe767f-d4c7-4c5a-8a24-31b846b744b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_cor_bypg\n",
    "hist_cor_bypg = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  Procedure:\n",
    "    populated_from: COPDGene\n",
    "    slot_derivations:\n",
    "      associated_participant:\n",
    "        populated_from: phv00159568\n",
    "      procedure_concept:\n",
    "        expr: \"'OMOP:4336464'\"  #coronary bypass graft\n",
    "      procedure_status:\n",
    "        populated_from: phv00159631\n",
    "        value_mappings:\n",
    "          '0': ABSENT\n",
    "          '1': PRESENT\n",
    "\"\"\")\n",
    "\n",
    "# Dump to YAML\n",
    "with open(var_dir + \"procedure/\" + \"hist_cor_bypg\" + \".yaml\", \"w\") as f:\n",
    "    yaml.dump(hist_cor_bypg, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f167b-0aa3-4838-93fd-d7c1647f8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the observations slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_procedures_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"procedures\", {})\n",
    "\n",
    "# Add the Demography object_derivation to the demography slot\n",
    "participant_procedures_slot.setdefault(\"object_derivations\", [ hist_cor_angio, hist_cor_bypg ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f40868-b270-4ceb-95d3-69e8d998d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edu_lvl\n",
    "edu_lvl = yaml.safe_load(\"\"\"\n",
    "class_derivations:\n",
    "  SdohObservation:\n",
    "    populated_from: COPDGene\n",
    "    slot_derivations:\n",
    "      associated_participant:\n",
    "        populated_from: phv00159568\n",
    "      category:\n",
    "        expr: \"'EDUCATIONAL_ATTAINMENT'\"\n",
    "      value_enum:\n",
    "        populated_from: phv00159773\n",
    "        value_mappings:\n",
    "          '1': 8TH_GRADE_OR_LESS\n",
    "          '2': HIGH_SCHOOL_NO_DIPLOMA\n",
    "          '3': HIGH_SCHOOL_GRADUATE_GED\n",
    "          '4': SOME_COLLEGE_OR_TECH_NO_DEGREE\n",
    "          '5': COLLEGE_OR_TECH_WITH_DEGREE\n",
    "          '6': MASTERS_OR_DOCTORAL_DEGREE\n",
    "\"\"\")\n",
    "\n",
    "# Dump to YAML\n",
    "with open(var_dir + \"sdoh_observation/\" + \"edu_lvl\" + \".yaml\", \"w\") as f:\n",
    "    yaml.dump(edu_lvl, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee7fba1-14a5-4250-8a7f-fe8fb5859d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the observations slot on Participants class\n",
    "participant_cls = participant_yaml.setdefault(\"class_derivations\", {}).setdefault(\"Participant\", {})\n",
    "participant_sdoh_observations_slot = participant_cls.setdefault(\"slot_derivations\", {}).setdefault(\"sdoh_observations\", {})\n",
    "\n",
    "# Add the Demography object_derivation to the demography slot\n",
    "participant_sdoh_observations_slot.setdefault(\"object_derivations\", [ edu_lvl ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070261a-440a-40ec-ad40-143c4cf59208",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_spec = person_yaml\n",
    "\n",
    "input_data = cur_row\n",
    "\n",
    "# Create ObjectTransformer and apply transformation\n",
    "transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "transformer.source_schemaview = SchemaView(source_schema)\n",
    "transformer.target_schemaview = SchemaView(target_schema)\n",
    "transformer.create_transformer_specification(transform_spec)\n",
    "\n",
    "# Transform all rows\n",
    "output_data = []\n",
    "for row in data_rows:\n",
    "    result = transformer.map_object(row, source_type=\"COPDGene\")\n",
    "    if result:  # Avoid None or empty dicts\n",
    "        output_data.append(result)\n",
    "\n",
    "# Final wrapped structure (key should match the collection slot, or be schema-compatible)\n",
    "wrapped_output = {\n",
    "    \"persons\": output_data\n",
    "}\n",
    "\n",
    "# Dump to YAML\n",
    "with open(\"transformed_person_data_DS_CS.yaml\", \"w\") as f:\n",
    "    yaml.dump(wrapped_output, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "print(\"Transformation Successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2751e283-9235-4423-b15e-d1922c612dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump final Person class to YAML\n",
    "with open(var_dir + \"person\" + \".yaml\", \"w\") as f:\n",
    "    yaml.dump(person_yaml, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5851b15-1e24-46cb-b95e-b7d50f656c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dm-bip)",
   "language": "python",
   "name": "dm-bip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
