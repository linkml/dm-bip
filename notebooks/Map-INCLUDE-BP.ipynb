{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf6739dc-7468-44b2-b00c-3ca089958005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import os\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import time\n",
    "import yaml\n",
    "\n",
    "from linkml.validator.loaders import TsvLoader\n",
    "from linkml_runtime import SchemaView\n",
    "from linkml_map.transformer.object_transformer import ObjectTransformer\n",
    "\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3790e2-4b96-4fa4-a347-8f7aa2371b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "\n",
    "    def __getitem__(self, pht_id):\n",
    "        file_path = os.path.join(self.base_path, f\"{entity}.tsv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"No TSV file found for {entity} at {file_path}\")\n",
    "        return TsvLoader(os.path.join(self.base_path, f\"{entity}.tsv\")).iter_instances()\n",
    "\n",
    "    def __contains__(self, pht_id):\n",
    "        return os.path.exists(os.path.join(self.base_path, f\"{entity}.tsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28678d48-14be-442a-be76-f51f305450f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_spec_files(directory, search_string):\n",
    "#     \"\"\"\n",
    "#     Find YAML files in the directory that contain the search_string.\n",
    "#     Returns a sorted list of matching file paths.\n",
    "#     \"\"\"\n",
    "#     directory = Path(directory)\n",
    "\n",
    "#     result = subprocess.run(\n",
    "#         ['grep', '-rl', search_string, str(directory)],\n",
    "#         stdout=subprocess.PIPE,\n",
    "#         text=True,\n",
    "#         check=True\n",
    "#     )\n",
    "\n",
    "#     file_paths = [\n",
    "#         Path(p.strip()) for p in result.stdout.strip().split('\\n')\n",
    "#         if p.strip().endswith(('.yaml', '.yml'))\n",
    "#     ]\n",
    "#     return sorted(file_paths, key=lambda p: p.stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77718894-041c-47e3-98ab-901c71b83aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multi_spec_transform(data_loader, spec_files, source_schema, target_schema):\n",
    "#     print(f'\\n** spec_files: {spec_files}')\n",
    "#     for file in spec_files:\n",
    "#         print(f\"{file.stem}\", end='', flush=True)\n",
    "#         try:\n",
    "#             with open(file) as f:\n",
    "#                 specs = yaml.safe_load(f)\n",
    "#                 print(f'\\n** specs: {specs}')\n",
    "#             for block in specs:\n",
    "#                 derivation = block[\"class_derivations\"]\n",
    "#                 print(\".\", end='', flush=True)\n",
    "#                 for class_name, class_spec in derivation.items():\n",
    "#                     pht_id = class_spec[\"populated_from\"]\n",
    "#                     rows = data_loader[pht_id]\n",
    "\n",
    "#                     transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "#                     transformer.source_schemaview = SchemaView(source_schema)\n",
    "#                     transformer.target_schemaview = SchemaView(target_schema)\n",
    "#                     transformer.create_transformer_specification(block)\n",
    "\n",
    "#                     for row in rows:\n",
    "#                         mapped = transformer.map_object(row, source_type=pht_id)\n",
    "#                         yield mapped\n",
    "#             print('')\n",
    "#         except Exception as e:\n",
    "#             print(f\"\\n⚠️  Error processing {file}: {e.__class__.__name__} - {e}\")\n",
    "#             print(block)\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d5f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "331e2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** data: [{'id': 1301, 'timepoint': 1, 'gender': '2', 'level_support': 1, 'race': '5', 'ethnicity': '2'}, {'id': 1302, 'timepoint': 1, 'gender': '2', 'level_support': 1, 'race': '6', 'ethnicity': '2'}, {'id': 1303, 'timepoint': 1, 'gender': '2', 'level_support': 1, 'race': '5', 'ethnicity': '2'}]\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#  FOR INCLUDE DATA  #\n",
    "######################\n",
    "\n",
    "\n",
    "def single_spec_transform(tsv_file, spec_file, source_schema, target_schema, target_class=\"Participant\"):\n",
    "    # Load your mapping spec\n",
    "    with open(spec_file) as f:\n",
    "        spec = yaml.safe_load(f)\n",
    "#         print(f'\\n** spec: {spec}')\n",
    "        \n",
    "#     # If spec is a list, merge all dicts\n",
    "#     if isinstance(spec, list):\n",
    "#         merged_spec = {}\n",
    "#         for block in spec:\n",
    "#             merged_spec.update(block)\n",
    "#     else:\n",
    "#         merged_spec = spec\n",
    "    \n",
    "    \n",
    "#     # Find all blocks that map to your target class\n",
    "#     class_blocks = [\n",
    "#         block[\"class_derivations\"][target_class]\n",
    "#         for block in spec\n",
    "#         if target_class in block.get(\"class_derivations\", {})\n",
    "#     ]\n",
    "#     if not class_blocks:\n",
    "#         raise ValueError(f\"{target_class} not found in any class_derivations in {spec_file}\")\n",
    "#     print(f'\\n** class_blocks: {class_blocks}')\n",
    "    \n",
    "#     class_spec = class_blocks[0] if class_blocks else None\n",
    "#     print(f'\\n** class_spec: {class_spec}')\n",
    "    \n",
    "    \n",
    "    # Load your TSV data\n",
    "    data = pd.read_csv(tsv_file, sep=\"\\t\").to_dict(orient=\"records\")\n",
    "    \n",
    "    # Cast field values to strings\n",
    "    fields_to_cast = ['gender', 'race', 'ethnicity']\n",
    "    data = [\n",
    "        {k: (str(v) if k in fields_to_cast and v is not None else v) for k, v in row.items()}\n",
    "        for row in data\n",
    "    ]\n",
    "    print(f'\\n** data: {data[:3]}')\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Another option to fix value_mappings - Set up the transformer once\n",
    "    transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "    transformer.source_schemaview = SchemaView(source_schema)\n",
    "    transformer.target_schemaview = SchemaView(target_schema)\n",
    "\n",
    "    results = []\n",
    "    for block in spec:\n",
    "        if 'class_derivations' in block and target_class in block['class_derivations']:\n",
    "            class_spec = block['class_derivations'][target_class]\n",
    "            # Load this class mapping spec\n",
    "            transformer.create_transformer_specification(block)\n",
    "            for row in data:\n",
    "                mapped = transformer.map_object(row, source_type=class_spec[\"populated_from\"])\n",
    "                results.append(mapped)\n",
    "            break  # Only process the first matching block\n",
    "\n",
    "    return results\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Set up the transformer\n",
    "#     transformer = ObjectTransformer(unrestricted_eval=True)\n",
    "#     transformer.source_schemaview = SchemaView(source_schema)\n",
    "#     transformer.target_schemaview = SchemaView(target_schema)\n",
    "#     transformer.create_transformer_specification(merged_spec)\n",
    "    \n",
    "    \n",
    "    # DEBUG\n",
    "#     print(\"\\n--- Mapping spec for sex field ---\")\n",
    "#     pprint.pprint(class_spec['slot_derivations'].get('sex'))\n",
    "#     print(\"\\n--- Entire Mapping spec ---\")\n",
    "#     pprint.pprint(merged_spec)\n",
    "\n",
    "    \n",
    "    # Map each row and yield the result\n",
    "#     row_count = 0\n",
    "#     for row in data:\n",
    "#         if row_count < 5:\n",
    "#             print(f'\\n** row: {row}')\n",
    "#         row_count += 1\n",
    "#         mapped = transformer.map_object(row, source_type=class_spec[\"populated_from\"])\n",
    "#         yield mapped\n",
    "\n",
    "# Example usage:\n",
    "results = list(single_spec_transform(\n",
    "    tsv_file=\"../data/BrainPower-STUDY/raw_data/TSV/demographics.tsv\",\n",
    "    spec_file=\"../data/BrainPower-STUDY/model_transformation/brain_power_transformation_PARTICIPANT-ONLY.yaml\",\n",
    "    source_schema=\"../data/BrainPower-STUDY/study_specific_model/BrainPower_INCLUDE_SCHEMA.yaml\",\n",
    "    target_schema=\"../data/BrainPower-STUDY/include_schema/include_schema.yaml\",\n",
    "    target_class=\"Participant\"  # or whatever your class is called\n",
    "))\n",
    "\n",
    "\n",
    "with open(\"demographics_transformed.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(results, f, sort_keys=False, allow_unicode=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695be24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc847e9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'linkml_map' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlinkml_map\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlinkml_map\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'linkml_map' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import linkml_map\n",
    "print(linkml_map.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeea722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e84a8a-bec7-4ff2-bb70-2d8d4259e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batched(iterable, batch_size):\n",
    "#     iterator = iter(iterable)\n",
    "#     for first in iterator:\n",
    "#         batch = [first, *islice(iterator, batch_size - 1)]\n",
    "#         yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66313a05-df35-47b5-9f94-e6c65c5600b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_sv = SchemaView(\"/sbgenomics/workspace/output/WHI/Schema_WHI_v12_c1/Schema_WHI_v12_c1.yaml\")\n",
    "source_sv = SchemaView(\"../data/BrainPower-STUDY/study_specific_model/BrainPower_INCLUDE_SCHEMA.yaml\")\n",
    "source_schema = source_sv.schema\n",
    "\n",
    "target_sv = SchemaView(\"../data/BrainPower-STUDY/include_schema/include_schema.yaml\")\n",
    "target_schema = target_sv.schema\n",
    "\n",
    "#var_dir = \"/sbgenomics/workspace/NHLBI-BDC-DMC-HV/priority_variables_transform/WHI-ingest/\"\n",
    "var_dir = \"../data/BrainPower-STUDY/raw_data/TSV/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c4c8b-a9c2-4414-8f42-46b4ee10c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base = \"../data\"\n",
    "study_dir = \"BrainPower-TEST\"\n",
    "os.makedirs(f\"{output_base}/{study_dir}/\", exist_ok=True)\n",
    "\n",
    "data_loader = DataLoader(\"../data/BrainPower-STUDY/raw_data/TSV/\")\n",
    "\n",
    "entities = [\n",
    "    #\"Study\",\n",
    "    \"Participant\",\n",
    "    #\"Condition\"\n",
    "]\n",
    "\n",
    "start = time.perf_counter()\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"Starting {entity}\")\n",
    "    #spec_files = get_spec_files(var_dir, f\" {entity}:\")\n",
    "    spec_files = [Path(\"../data/BrainPower-STUDY/model_transformation/brain_power_transformation.yaml\")]\n",
    "    output_path = f\"{output_base}/{study_dir}/{entity}-data.yaml\"\n",
    "\n",
    "    # subset = spec_files\n",
    "#     subset = [p for p in spec_files if p.stem == \"demo\"]\n",
    "    # subset = [p for p in spec_files if p.stem >= \"stroke\"]\n",
    "    # subset = [p for p in spec_files if p.stem > \"afib\"]\n",
    "    subset = spec_files\n",
    "\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        for batch in batched(multi_spec_transform(data_loader, subset, source_schema, target_schema), batch_size=100):\n",
    "            yaml.dump_all(batch, f, explicit_start=True)\n",
    "#         for batch in batched(multi_spec_transform(data_loader, mapping_spec_file, source_schema, target_schema), batch_size=100):\n",
    "#                     yaml.dump_all(batch, f, explicit_start=True)\n",
    "    \n",
    "        print(f\"{entity} Complete\")\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(f\"Time: {end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3a975-39dc-4ae2-8eb9-751bc7eb454c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cc0347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5555c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Read your input data file (demographics.tsv)\n",
    "# df = pd.read_csv(\"../data/BrainPower-STUDY/raw_data/TSV/demographics.tsv\", sep=\"\\t\")\n",
    "# data_records = df.to_dict(orient=\"records\")\n",
    "# display(df.head())\n",
    "\n",
    "# # Step 2: Set up your transformation config\n",
    "# source_schema = \"../data/BrainPower-STUDY/study_specific_model/BrainPower_INCLUDE_SCHEMA.yaml\"\n",
    "# map_yaml = \"../data/BrainPower-STUDY/model_transformation/brain_power_transformation.yaml\"\n",
    "# target_schema = \"../data/BrainPower-STUDY/include_schema/include_schema.yaml\"\n",
    "\n",
    "\n",
    "# # Step 3: Initialize the transformer\n",
    "# ot = ObjectTransformer(\n",
    "#     source_schema_path=source_schema,\n",
    "#     map_path=map_yaml,\n",
    "#     target_schema_path=target_schema,\n",
    "#     output_path=\"../data/BrainPower-STUDY/transformed_data/demographics_transformed.yaml\",  # or .json as desired\n",
    "#     input_format=\"tsv\",\n",
    "#     target_class=\"Participant\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Transform and write output\n",
    "# transformed = ot.transform_file(\"../data/BrainPower-STUDY/raw_data/TSV/demographics.tsv\")\n",
    "\n",
    "\n",
    "# with open(\"demographics_transformed.yaml\", \"w\") as f:\n",
    "#     yaml.safe_dump(transformed, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "# print(\"Transformed data written to demographics_transformed.yaml\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d174f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (linkml-env)",
   "language": "python",
   "name": "linkml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
